# progress report

I downloaded the ollama version of llama 3 and got so far that I could let two instances of Llama3.1 communicate with
each other, as well as generating a number of possible answers to choose from. It works on one machine, and I am now at
a
point where I can start to think about how to use sentiment analysis on the first actors message and obtain their
polarity.

Also, I do still need to look into what kind of format I need for the input of the sentiment analysis. Running
a BERT like model should not be the problem, obtaining one that can do more than binary sentiment analysis might be a
bit more complicated. Alternatively I will have to find a way to frame the problem in a binary way.
I want to have a look at theoretical settings via System prompts, currently I put both actors at opposing ends of an
argument and just let them talk. I also noticed that, if I do not limit the number of tokens that are generated, the
runtime of course gets exponentially bad.
Another thing is, that if I want to actually do any selection via RL I will have to find a way to describe similarity of
answers, or rather to define similarity to the optimal answer constructed by the RL model. Alternatively I have two
ideas.

1. I could simply generate a target polarity state of the optimal answer and try to match that. Problem: What should
   that be based on?? A simple binary sentiment analysis is not really useful for this task.
2. Second is a bit more complicated and cumbersome, in the sense that I would have to do more exploration by trying to
   learn to predict the sentiment that results from the answers. To do this I would have to actually feed the answers
   into the first actor. Even if I were to do that, as this is not a deterministic process I would have to feed it in
   multiple times which is simply not feasible computation wise.

Either approach presents me with new problems I will have to think about. We assume that the states are the polarity we
obtain from the input messages from the first actor. If this is simply a binary sentiment analysis, then the state space
will be too simple. However, as far as I know, there are no well-trained other options as far as polarity goes. I know
of models trained on emotion datasets that I have used before while working on DLNLP projects. Maybe I will just look
into that and work with it for now.

I want to clean up the code I've written so far and maybe look for a better way to handle passing along the history of
previous interactions. Either by limiting the context size or applying some sort of summarization scheme.

# What are the next steps?!
- Clean up the code
- DONE Look into sentiment analysis models 
- Think about how to handle the history of interactions
- Look into theoretical settings via System prompts
- Think about how to define similarity of answers
- DONE Think about how to define similarity to the optimal answer
- KINDA DONE Think about how to define the target polarity state of the optimal answer
- Think about how to learn to predict the sentiment that results from the answers
- KINDA DONE Think about how to handle the state space
- Look into emotion datasets
- Look into summarization schemes
- Look into how to handle the context size
- Look into how to handle the runtime of the model